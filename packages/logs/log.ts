import { Axiom } from '@axiomhq/js';
import { config, env } from '@repo/config';
import pino from 'pino';
import {
  GlobalContextStorageProvider,
  lambdaRequestTracker,
  pinoLambdaDestination,
} from 'pino-lambda';
import pretty from 'pino-pretty';

// ---------- DESTINATIONS ----------
/** Our axiom client */
const axiom = (env as Record<string, string>).AXIOM_TOKEN
  ? new Axiom({
      token: (env as Record<string, string>).AXIOM_TOKEN,
    })
  : undefined;
/** Our lambda destination (ensures things are formatted for cloudwatch) */
const lambdaDest = pinoLambdaDestination();
/** Our pretty destination (ensures things are formatted for the console) */
const prettyDest = pretty({
  colorize: true,
  translateTime: 'SYS:HH:mm:ss',
  // Ignore some params that we don't care about locally
  ignore:
    'env,userId,request,langfuseTraceId,awsRequestId,apiRequestId,x-correlation-id,x-correlation-trace-id',
});

// ---------- HELPERS ----------
/** Adds lambda request context to the current request context */
export const addLambdaRequestContext = lambdaRequestTracker();

type LogMetadata = {
  langfuseTraceId?: string;
  userId?: string;
  awsRequestId?: string;
  request?: { method?: string; path?: string };
};

/** Gets the current request context */
export const getLogMetadata = () =>
  ({ ...GlobalContextStorageProvider.getContext() }) as LogMetadata;

/** Adds metadata to the current request context */
export const addLogMetadata = (metadata: Record<string, unknown>) => {
  GlobalContextStorageProvider.updateContext(metadata);
};

/** Ensures all logs are flushed */
export const flushLogs = async () => {
  try {
    if (axiom) {
      await axiom.flush();
    }
  } catch (error) {
    console.error('[log] Failed to flush logs:', error);
  }
};

/** The message format generated by pino */
type PinoLog = {
  level: string;
  msg: string;
  [key: string]: unknown;
};

// ---------- PINO ----------
/** Create a custom destination that handles all our destinations */
const customDestination = {
  write: (payload: string) => {
    try {
      // When we're running in AWS, we need to do a few things
      if (config.isAWS) {
        // We should structure the logs for cloudwatch
        lambdaDest.write(payload);

        // Since we can't see the logs easily, we should also send logs to Axiom (if it's configured)
        if (axiom) {
          // Format the payload to be the way axiom expects it
          const {
            time,
            level,
            msg,
            // biome-ignore lint/correctness/noUnusedVariables: We need to extract this to avoid passing it
            'x-correlation-id': correlationId,
            // biome-ignore lint/correctness/noUnusedVariables: We need to extract this to avoid passing it
            'x-correlation-trace-id': traceId,
            // Grab the rest of the context
            ...context
          } = JSON.parse(payload) as PinoLog;
          axiom.ingest(env.AXIOM_DATASET, [
            {
              // Axiom-specific fields (we have to include level twice or it doesn't work right)
              _time: time,
              level,
              severity: level,
              message: msg,
              // Remaining context
              ...context,
            },
          ]);
        }
      }
      // If we're running locally, show the pretty output
      else {
        prettyDest.write(payload);
      }
    } catch (error) {
      console.error('[log] Failed to handle log:', error);
    }
  },
};

// Create our logging instance
export const log = pino(
  {
    level: process.env.LOG_LEVEL || 'info',
    formatters: {
      level: (label) => ({ level: label }),
    },
    // Attach any global context variables
    base: {
      env: config.stage,
    },
    // Add any additional global context into our logs
    mixin: () => getLogMetadata(),
  },
  customDestination
);
